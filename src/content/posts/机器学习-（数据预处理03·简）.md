---
title: 机器学习-（数据预处理03·简）
description: 使用管道pipeline进行数据预处理
pubDate: 2025-07-05
---

## Pipeline 管道·概述

将多个数据处理步骤串联起来的工具，用于自动化和标准化从原始数据到模型输入的整个转换流程。它能显著提升代码的可读性、可维护性，并避免常见的数据泄露问题。

### 优点

- **清理代码：** 在预处理的每个步骤中数据可能会变得混乱。使用管道，无需在每个步骤中手动跟踪训练和验证数据。
- **错误少：** 误应用步骤或忘记预处理可能性降低。
- **更容易生产：** 将模型从原型过渡到可大规模部署的模型可能非常困难。但管道可以提供帮助。
- **模型验证的更多选项：** 可以使用交叉验证。
- **超参数调优**：可对整个管道中的所有步骤进行统一调参。
- **简化流程**：将数据清洗、特征工程、模型训练等步骤整合为单一对象。
- **防止数据泄露**：确保预处理（如标准化、特征选择）在训练集和测试集上的应用方式一致。

## 构建步骤

### 导入数据集

导入数据集并分出测试集和训练集

```python
import pandas as pd
from sklearn.model_selection import train_test_split

data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')
y = data.Price
X = data.drop(['Price'], axis=1)

X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)

categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == "object"]

numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]

my_cols = categorical_cols + numerical_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()
```

### 定义预处理

将处理缺失值和分类变量（给属性值编码）两个预处理步骤绑定在一起

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# 定义数值列的转换器：填充固定值
numerical_transformer = SimpleImputer(strategy='constant')

# 定义一个管道，按顺序执行步骤：用最频繁值填充缺失值，定义一个one-hot编码器
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 定义一个转换器：对DataFrame中不同的列并行应用不同的转换器：对数值列应用数值列转换器；对分类列应用分类列转换器（管道）
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
      
    ],
		remainder='drop'  # 对未指定的列的处理方式（默认'drop'，可选'passthrough'保留)
)
```

```python
ColumnTransformer(transformers)
```

该类的transformers参数接收一个元组列表，每个元组列表定义一个转换步骤

**格式**：(名称, 转换器, 列名/索引)

- **名称**：自定义的步骤名称（字符串），用于后续引用。
- **转换器**：实现 `fit` 和 `transform` 方法的对象（如 `StandardScaler`、`OneHotEncoder`）。
- **列名 / 索引**：指定要应用该转换的列，可以是列名列表、整数索引列表或布尔掩码。

### 定义模型

使用随机森林定义一个模型

```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)
```

##### 其中

1. n_estimators=100

   随机森林中决策树数量是100，一般从100开始尝试。过多可能会过拟合，但是提升数量可能增加模型精度。

2. random_state=0

   随机数生成器种子值设置为0。固定此值可确保每次运行模型时的随机过程（如特征采样、数据分割）完全相同，结果可复现。便于调整其他参数。

##### 其他重要参数

1. `max_depth=None`

- 决策树的最大深度。
- 若为 `None`，树会生长到所有叶子节点纯（或样本数小于 `min_samples_split`）。

**作用**：限制深度可防止过拟合，但可能欠拟合。

2. `min_samples_split=2`

- 分裂内部节点所需的最小样本数。
- 值越大，树越简单，抗过拟合能力越强。

3. `min_samples_leaf=1`

- 叶子节点所需的最小样本数。
- 避免生成过于具体的叶子节点，防止过拟合。

4. `max_features='auto'`

分裂时考虑的最大特征数。

- 对回归问题，默认 `max_features=n_features`（使用所有特征）。
- 减小此值可增加树之间的多样性，降低过拟合。

### 创建和评估管道

使用Pipeline类来定义一个捆绑预处理和建模步骤的管道。

```python
from sklearn.metrics import mean_absolute_error

# 定义管道，捆绑预处理和模型
my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', model)
                             ])
# 使用管道拟合模型
my_pipeline.fit(X_train, y_train)

# 预测。管道会在生成预测之前自动预处理这些特征
preds = my_pipeline.predict(X_valid)

# 模型评估
score = mean_absolute_error(y_valid, preds)
print('MAE:', score)
```

### 总结

使用管道可以更方便的进行数据预处理，如填充缺失值和分类变量，在此之前只需要将列进行分类（如分为数值列和分类列，还可以分的更细）。

然后对不同类型的列创建预处理器，对于分类列预处理器中需要两个步骤（填充缺失值，分类变量）需要使用Pipeline。

然后定义模型。

然后使用Pipeline将预处理器和模型捆绑。

拟合模型和预测模型时只需要将特征填入，不需要再对每个特征单独进行预处理了。

使预处理和模型训练和预测过程更加流畅，减少错误，更加清晰明了，定义起来也更加方便。