---
title: 机器学习-（决策树&随机森林·简）
description: 简述决策树与随机森林算法原理，通过简单决策树构建理解机器学习过程
pubDate: 2025-06-30
---

### 决策树过程简述

#### 决策树：

一个二叉树，每个节点会有一个判断，如房屋面积>100m^2

是或否会将决策过程带入更深一层的子节点，依次进行判断。

所以，一个模型的好坏，一定程度上取决于决策树构建的是否合理以及其深度。深度越深，捕获的特征也就越多，模型也就可能越好。而模型的构建就与算法和数据有关。

决策树的叶子节点就是我们最后预测的结果（label）：分类结果或回归结果

### 房地产价格预测

以房地产价格预测问题为例。我们先用人脑学习一下过往房地产价格形成一个模式（模型），然后预测未来房地产价格。

提取特征：我们注意到卧室数量越多，房屋价格越高，于是我们可以提取这个特征，构建第一层决策树；但是我们还注意到，地块大小，房屋面积，位置，浴室个数甚至房屋楼层高度等都会影响房地产价格，浴室我们在此基础上继续加深我们的决策树。

### 决策树代码实现

##### scikit-learn+pandas+kaggle Melbourne Housing Snapshot数据集为例实现

#### 数据处理&定义模型

引入pandas包和sklearn.tree中的”决策树回归“模块

引入sklearn.modle_selection中的train_test_split模块

```python
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
```

定义数据集文件路径，使用pandas打开数据集为Dataframe

```python
melbourne_file_path = \
	'../dansbecker/melbourne-housing-snapshot/melb_data.csv'
```

去除一行中有空值的样本

```python
melbourne_data.dropna(axis=0)
```

定义标记（Series）和特征（Datafrme）

```python
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]
```

留出训练集和测试集，我们需要使用未见数据验证模型，如果使用同样的数据（训练和测试数据相同）验证模型会过于乐观导致失败结果

```python
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
```

定义决策树回归模型

```python
melbourne_model = DecisionTreeRegressor()
```

#### 拟合模型

```python
melbourne_model.fit(train_X, train_y)
```

#### 模型预测

利用模型和测试集特征生成目标值

```python
predicted_home_prices = melbourne_model.predict(val_X)
```

#### 模型评估

引入sklearn.metrics包中的mean_absolute_error模块（绝对平均误差）

用来简单衡量模型的定义和拟合效果

```python
from sklearn.metrics import mean_absolute_error
```

计算MAE指标

```python
print(mean_absolute_error(val_y, predicted_home_prices))
```

### 过拟合&欠拟合

#### 过拟合

在该例子中，我们树的深度有限，在定义模型时可以给定深度参数，深度越深意味着我们捕获了越多的特征，于是分配给每个叶子节点的房屋数量就会越少，也就是模型和训练数据拟合的更加精确。但是使用测试集测试可能预测结果表现会非常不好，这是由于训练数据量是有限的或者说不够多

#### 欠拟合

在该例子中，如果决策树深度过浅，导致大量的房屋被分成一类，尽管其可能有诸多不同，这是因为我们捕获的特征数量过少，没有捕获到房地产价格区别的重要原因和模式或区别。导致在训练集和测试集中的表现或拟合度都不理想。

### 模型优化

我们希望找到欠拟合和过拟合之间最佳的平衡点，以达到MAE指标最小。

对于本例来说，决策树深度越深（叶子节点越多），模型拟合度越高。

所以我们从少数量的叶子过渡到更多数量的叶子，也就是从欠拟合向过拟合过渡，在这个过程中间应该有某一点MAE最小（凹函数顶点）

在scikit-learn中决策树回归模型定义中可以增加max_leaf_nodes参数限制最大叶子数；该函数返回不同拟合度（叶子节点数）的MAE

```python
def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae)
```

查看优化结果，选择参数

```python
for max_leaf_nodes in [5, 50, 500, 5000]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, my_mae))
```

发现参数为500时拟合效果最好（只是在当前设置情况下）

### 随机森林

随机森林是一种集成学习，”三个臭皮匠，顶个诸葛亮“。

他的基本组成就是决策树。

通过每次随机的从样本空间中抽取一定数量的样本（小于样本空间）构建决策树，决策树每次进行特征捕获时也只选择选择样本中的一部分。

如此循环构建N颗决策树。

进行预测时，根据多棵决策树的结果进行加权平均或等算法（回归）给出结果，或将投票数量最多的结果作为预测结果（分类）

### 随机森林代码实现

##### scikit-learn（RandomForestRegressor）+pandas实现

#### 数据处理&定义模型

引入随机森林回归模块和MAE评估模块，训练集测试集留出模块与pandas

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
import pandas as pd
```

数据处理

```python
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
melbourne_data = melbourne_data.dropna(axis=0)
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                        'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)
forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
```

#### 拟合模型&预测&评估

```python
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```

发现预测的结果是要比单纯的决策树要好的