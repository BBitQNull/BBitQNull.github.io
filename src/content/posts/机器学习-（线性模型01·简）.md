---
title: 机器学习-（线性模型01·简）
description: 简述线性模型，多元线性回归与最小二乘法，岭回归，python实现
pubDate: 2025-07-01
---

### 线性模型

#### 基本过程

线性模型具有高度可解释性，可以清楚地看到不同特征对目标值的影响。
$$
f(x)=\beta_1x_1+\beta_2x_2+\beta_3x_3+\cdots+\beta_dx_d+\beta_0
$$
其中$x$是样本的特征，$x_i$表示是第几个样本，而$\beta$是学习后模型的参数，$\beta_i$表示第$i$个特征的权重，将训练集中的样本特征$x$和对应目标值$y$带入进行计算求解$\beta$，拟合出一条整体损失最小的直线即是线性模型

#### 矩阵表示

对于多元线性回归一般采用矩阵表示。

##### 定义

样本数量：m

特征数量：d

权重向量coef_：$\vec{\beta}=\{\beta_1;\beta_2;\cdots;\beta_d\}$

偏置系数intercept_：$\beta_0$

$\hat{\beta}=\{\vec{\beta};\beta_0\}$

数据特征矩阵$\vec{X}$：
$$
\begin{pmatrix}
x_{11}&x_{12}&x_{13}&\cdots&x_{1d}&1\\
x_{21}&x_{22}&x_{23}&\cdots&x_{2d}&1\\
\vdots\\
x_{m1}&x_{m2}&x_{m3}&\cdots&x_{md}&1
\end{pmatrix}
$$
$\vec{X}\hat{\beta}=\vec{f}(x)$（列向量）

### 普通最小二乘法

最小二乘法是多元线性回归模型中权重向量求取的一种常见方法。

通过最小化预测值和真实值的误差平方和（SSE）求取最佳权重向量。

##### coef_=

$$
argmin(\vec{y}-\vec{X}\hat{\beta})^T(\vec{y}-\vec{X}\hat{\beta})
$$

上式的意思就是求取最小平方和（SSE）时的权重向量

##### 计算

通过对$\hat{\beta}$求偏导得到
$$
\frac{\partial{E_{\hat{\beta}}}}{\partial{\hat{\beta}}}=2\vec{X}^T(\vec{X}\hat{\beta}-\vec{y})
$$
令其等于0得到
$$
（\vec{X}^T\vec{X}）^{-1}\vec{X}^Ty
$$
该式子要求$\vec{X}^T\vec{X}$为满秩矩阵或正定矩阵

#### 代码实现

scikit-learn实现

```python
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
```

查看权重向量和偏置值

```python
reg.coef_
reg.intercept_
```

### 非负最小二乘法

线性模型的权重向量中有非负要求（回归系数有正约束）

#### 代码实现

定义模型时将参数positive设置为True即可

```python
from sklearn.linear_model import LinearRegression
reg = LinearRegression(positive=True)
reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
```

### 岭回归

#### 最小二乘法的问题

##### 求逆不稳定

在最小二乘法的最后的计算中，如果$\vec{X}^T\vec{X}$没有逆矩阵，则无法求解，也就要求该矩阵不是奇异矩阵。

但当自变量也就是特征之间有多重共线性时（特征相关或特征数大于样本数），该矩阵就会趋近奇异，导致是否可求逆不稳定。

##### 方差变大

矩阵的列向量张成的空间维度低于变量数，微小的数据扰动会导致最优解在共线方向大幅便宜

##### 泛化能力差

导致预测结果很差

#### 岭回归Ridge

为了解决传统最小二乘法求解多元线性回归的问题（自变量线性相关，小样本）。提出了岭回归。

在一定程度上放弃了最小二乘法的无偏性，以损失部分信息，降低精度来攻克病态矩阵问题的回归方法。

可以有效的处理小样本问题，自变量线性相关问题，应对过拟合问题，且具有更好的预测性能（泛化能力）

##### 范数

可以看作距离

##### L1范数

也叫做曼哈顿距离，即向量个元素的绝对值之和

##### L2范数

向量中各元素平方和的平方根，欧几里得距离就是L2范数

##### 与最小二乘法的不同

岭回归在损失函数中增加了L2范数，最终得到coef_为
$$
（（\vec{X}^T\vec{X}+\lambda I）^{-1}\vec{X}^Ty）
$$
其中$\lambda$（sklearn中的alpha）是正则化系数或惩罚系数，对模型复杂度起到调节作用，而$I$是单位矩阵。

#### 代码实现

定义模型时的alpha参数控制惩罚程度。

当alpha越大，模型复杂度越低，alpha越小，模型复杂度越高（越接近最小二乘法）。

```bash
过拟合<-----alpha----->欠拟合(泛化)
```

```python
from sklearn.linear_model import Ridge
reg = linear_model.Ridge(alpha=.5)
reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
```

查看权重矩阵和偏置值

```python
reg.coef_
reg.intercept_
```

Ridge模型可以指定solver参数，用于指定求解器

|  solver   |                condition                |
| :-------: | :-------------------------------------: |
|   auto    |       根据数据规模和类型自动选择        |
| cholesky  |                密集矩阵                 |
| sparse_cg |            稀疏矩阵&快速求解            |
|   lsqr    |          稀疏矩阵｜大规模问题           |
|    svd    |              数值稳定性高               |
|    sag    |    大规模数据集（样本数 >> 特征数）     |
|   saga    | 大规模数据集且需要弹性网支持，改进于sag |

